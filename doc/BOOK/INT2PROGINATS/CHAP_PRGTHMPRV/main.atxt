%{
//
dynload "libatsdoc/dynloadall.dats"
//
#include "./../MYTEXT/int2proginats.dats"
//
%}\
#comment("\n\
The file is automatically generated by [atsdoc] from main.atxt.\n\
")
#comment("\n\
Time of Generation: #timestamp()\
")
<chapter
id="programming_with_theorem-proving">
#title("Programming with Theorem-Proving")

#para("\

#emphasis("Programming with Theorem-Proving") (PwTP) is a rich and broad
programming paradigm that allows cohesive construction of programs and
proofs in a syntactically intwined manner. The support for PwTP in ATS is a
signatory feature of ATS, and the novelty of ATS largely stems from it. For
people who are familiar with the so-called Curry-Howard isomorphism, I
emphasize that PwTP as is supported in ATS makes little, if any, essential
use of this isomorphism (between proofs and programs): The dynamics of ATS
in which programs are written is certainly not pure and the proofs encoded
in ATS/LF are not required to be constructive, either. However, that proof
construction in ATS can be done in a style of (functional) programming is
fundamentally important in terms of syntax design for ATS, for the need to
combine programs with proofs would otherwise be greatly more challenging.

")

#para("\
In this chapter, I will present some simple but convincing examples to
illustrate the power and flexibility of PwTP as is supported in
ATS. However, the real showcase for PwTP will not arrive until after the
introduction of linear types in ATS, when linear proofs can be combined
with programs to track and safely manipulate resources such as memory and
objects (e.g, file handles). In particular, PwTP is to form the cornersone
of the support for imperative programming in ATS.
")

#para("\
Please find #mycodelink("CHAP_PRGTHMPRV/", "on-line")
the code employed for illustration in this chapter plus some additional
code for testing.\
")

<!-- ****** ****** -->

<sect1
id="circumventing_nonlinear_constraints">
#title("Circumventing Nonlinear Constraints")

#para("
The constraint-solver of ATS is of rather diminished power. In particular,
constraints containing nonlinear integer terms (e.g., those involving the
use of multiplication (of variables)) are immediately rejected. This
weakness must be properly addressed for otherwise it would become a
crippling limitation on practicality of the type system of ATS. I now use
a simple example to demonstrate how theorem-proving can be employed to
circumvent the need for handling nonlinear constraints directly.
")#comment("para")

#para("\

A function template #dyncode("list_concat") is implemented as follows:

#dats2xhtml("\
//
// [list_concat] does typecheck in ATS2
// [list_concat] does not typecheck in ATS1
//
fun{
a:t@ype
} list_concat{m,n:nat}
(
  xss: list (list (a, n), m)
) : list (a, m * n) =
  case+ xss of
  | list_nil () => list_nil ()
  | list_cons (xs, xss) => list_append<a> (xs, list_concat xss)
// end of [list_concat]
")

where the interface for #dyncode("list_append") is given below:

#sats2xhtml("\
fun{
a:t@ype
} list_append {n1,n2:nat}
  (xs: list (a, n1), ys: list (a, n2)): list (a, n1+n2)
")

Given a list #dyncode("xss") of length #stacode("m") in which each element is of
the type #stacode("list(T,n)") for some type T,
#dyncode("list_concat&lt;T&gt;(xss)") constructs a list of the type
#stacode("list(T,m*n)"). When the first matching clause in the code for
#dyncode("list_concat") is typechecked, a constraint is generated that is
essentially like the following one:

#atscode("\
m = m1 + 1 implying n + (m1 * n) = m * n holds for all natural numbers m, m1 and n.
")

This contraint may look simple, but it was once rejected by the ATS
constraint solver as it contains nonlinear integer terms (e.g.,
#stacode("m1*n") and #stacode("m*n")). In order to overcome (or rather
circumvent) the limitation, we can make use of theorem-proving. Another
implementation of #dyncode("list_concat") is given as follows:

#dats2xhtml("\
fun{
a:t@ype
} list_concat{m,n:nat}
(
  xss: list(list(a, n), m)
) : [p:nat] (MUL(m, n, p) | list(a, p)) =
(
//
case+ xss of
| list_nil () =>
    (MULbas() | list_nil())
| list_cons (xs, xss) => let
    val (pf | res) = list_concat (xss)
  in
    (MULind pf | list_append<a> (xs, res))
  end // end of [list_cons]
//
) (* end of [list_concat] *)
")

Given a list #dyncode("xss") of the type #stacode("list(list(T,n),m)"),
#dyncode("list_concat(xss)") now returns a pair #dyncode("(pf | res)") such
that #dyncode("pf") is a proof of the prop-type #stacode("MUL(m,n,p)") for
some natural number #stacode("p") and #dyncode("res") is a list of the type
#stacode("list(T,p)"), where the symbol bar (|) is used to separate proofs
from values. In other words, #dyncode("pf") acts as a witness to the
equality #stacode("p=m*n"). After proof erasure is performed, this
implementation of #dyncode("list_concat") is essentially translated into
the previous one (as far as dynamic semantics is concerned). In particular,
there is no need for proof construction at run-time.\

")#comment("para")

</sect1><!--"circumventing_nonlinear_constraints"-->

<!-- ****** ****** -->

<sect1
id="example_safe_matrix_subscripting">
#title("Example: Safe Matrix Subscripting")

#para("\

Internally, a matrix of the dimension m by n is represented as an array
of the size m*n. For matrix subscripting, we need to implement a function
template of the following interface:

#sats2xhtml("\
fun{
a:t@ype
} matrix_get
  {m,n:int}{i,j:nat | i < m; j < n}
  (A: arrayref (a, m*n), col: int n, i: int i, j: int j): (a)
// end of [matrix_get]
")

Assume that the matrix is represented in the row-major style. Then the
element indexed by i and j in the matrix is the element indexed by i*n + j
in the array that represents the matrix, where i and j are natural numbers
less than m and n, respectively. However, the following implementation
fails to pass typechecking:

#dats2xhtml("\
implement
{a}(*tmp*)
matrix_get (A, n, i, j) = A[i*n+j] // it fails to typecheck!!!
")

The simple reason for this failure is due to the ATS constraint solver not
being able to automatically verify that i*n+j is a natural number strictly
less than m*n. An implementation of #dyncode("matrix_get") that typechecks
can be given as follows:

#dats2xhtml("\
implement
{a}(*tmp*)
matrix_get
  {m,n}{i,j}
  (A, n, i, j) = let
//
  val (pf | _in_) = imul2 (i, n)
//
  prval ((*void*)) = mul_elim(pf)
  prval ((*void*)) = mul_nat_nat_nat(pf)
  prval ((*void*)) = mul_gte_gte_gte{m-1-i,n}()
//
in
  A[_in_+j]
end // end of [matrix_get]
")

where the functions called in the body of #dyncode("matrix_get")
are assigned the following interfaces:

#sats2xhtml("\
//
fun
imul2{i,j:int}
  (int i, int j):<> [ij:int] (MUL(i, j, ij) | int ij)
//
prfun
mul_elim
  {i,j:int}{ij:int} (pf: MUL(i, j, ij)): [i*j==ij] void
//
prfun
mul_nat_nat_nat
  {i,j:nat}{ij:int} (pf: MUL(i, j, ij)): [ij >= 0] void
//
prfun
mul_gte_gte_gte
  {m,n:int | m >= 0; n >= 0} ((*void*)): [m*n >= 0] void
//
")

Assume that m and n are natural numbers and i and j are natural numbers
less than m and n, respectively.  The proof code employed in the
implementation of #dyncode("matrix_get") to show i*n+j &lt; m*n proves
(m-1-i)*n &gt;= 0, which clearly implies m*n >= i*n+n > i*n+j.

")#comment("para")

#para("

Note that there are a variety of proof functions declared in
#myatscodelink("prelude/SATS/arith_prf.sats", "arith_prf.sats") for helping prove
theorems involving arithmetic operations. For examples of proof
construction in ATS, please find the implementation of some of these proof
functions in #myatscodelink("prelude/DATS/arith_prf.dats", "arith_prf.dats").\

")#comment("para")

#para("\

The entirety of the above presented code is available
#mycodelink("CHAP_PRGTHMPRV/matget.dats", "on-line").\

")#comment("para")

</sect1><!--"example_safe_matrix_subscripting"-->

<!-- ****** ****** -->

<sect1
id="specifying_with_precision"
xreflabel="specifying with enhanced precision">
#title("Specifying with Enhanced Precision")

#para("
The integer addition function can be assigned the following
(dependent) type in ATS to indicate that it returns the sum of
its two integer arguments:

#atscode("\
{i,j:int} (int(i), int(j)) -> int(i+j)
")

This type gives a full specification of integer addition as the only
(terminating) function that can be given the type is the integer addition
function. However, the factorial function, which yields the product of the
first n positive integers when applied to a natural number n, cannot be
given the following type:

#atscode("{n:int | n >= 0} int(n) -> int(fact(n))")

as #stacode("fact"), which refers to the factorial function, does not exist in
the statics of ATS. Evidently, a highly interesting and relevant question is
whether a type can be formed in ATS that fully captures the functional
relation specified by #stacode("fact")? The answer is affirmative. We can not
only construct such a type but also assign it to a (terminating) function
implemented in ATS.\

")#comment("para")

#para("\
Let us recall that the factorial function can be defined by the following
two equations:

#atscode("\
fact(0) = 1
fact(n) = n * fact(n-1) (for all n > 0)
")

Naturally, these equations can be encoded by the constructors associated
with the dataprop #stacode("FACT") declared as follows:

#sats2xhtml("\
dataprop
FACT(int, int) =
  | FACTbas(0, 1)
  | {n:nat}{r1,r:int}
    FACTind(n, r) of (FACT(n-1, r1), MUL(n, r1, r))
// end of [FACT]
")

Note that for any given natural number n and integer r, #stacode("FACT(n,
r)") can be assigned to a proof if and only if #dyncode("fact(n)") equals
r. Therefore, the following type:

#atscode("\
{n:nat} int(n) -> [r:int] (FACT(n, r) | int(r))
")

can only be assigned to a function that, if applied to a natural number n,
returns a proof and an integer such that the proof attests to the integer
being equal to #dyncode("fact(n)"). For instance, the following defined
function #dyncode("ifact") is assigned this type:

#dats2xhtml("\
//
fun
ifact
{n:nat} .<n>.
(
  n: int(n)
) :<> [r:int] (FACT(n, r) | int r) =
(
//
if
n = 0
then (FACTbas() | 1)
else let
  val (pf1 | r1) = ifact (n-1) // pf1: FACT(n-1, r1)
  val (pfmul | r) = imul2 (n, r1) // pfmul: FACT(n, r1, r)
in
  (FACTind(pf1, pfmul) | r)
end // end of [else]
//
) (* end of [ifact] *)
//
")

After proof erasure, #dyncode("ifact") precisely implements the factorial
function.

")#comment("para")

#para("\

Please find the entirety of the above presented code plus some testing code
#mycodelink("CHAP_PRGTHMPRV/ifact.dats", "on-line").

")

</sect1><!--"specifying_with_precision"-->

<!-- ****** ****** -->

<sect1
id="example_another_verified_factorial">
#title("Example: Another Verified Factorial")

#para("\

The function #dyncode("ifact") presented in the section on <xref
linkend=\"specifying_with_precision\"/> is a verified implementation of
the factorial function as its type guarantees that #dyncode("ifact")
implements the specification of factorial encoded by the dataprop
#stacode("FACT"). Clearly, the implementation of #dyncode("ifact") closely
follows the declaration of #stacode("FACT"). If we think of the latter as a
logic program, then the former is essentially a functional version
extracted from the logic program. However, the implementation of a
specification in practice can often digress far from the specification
algorithmically. For instance, we may want to have a verified
implementation of factorial that is also tail-recursive. This can be done
as follows:

#dats2xhtml("\
fun
ifact2
{n:nat} .<>.
(
  n: int (n)
) :<> [r:int] (FACT(n, r) | int r) = let
  fun loop
    {i:nat|i <= n}{r:int} .<n-i>.
  (
    pf: FACT(i, r)
  | n: int n, i: int i, r: int r
  ) :<> [r:int] (FACT(n, r) | int r) =
    if n - i > 0 then let
      val (pfmul | r1) = imul2 (i+1, r) in loop (FACTind(pf, pfmul) | n, i+1, r1)
    end else (pf | r) // end of [if]
  // end of [loop]
in
  loop (FACTbas() | n, 0, 1)
end // end of [ifact2]
")

The function #dyncode("ifact2") is assigned a type indicating that
#dyncode("ifact2") is a verified implementation of factorial, and it is
defined as a call to the inner function #dyncode("loop") that is clearly
tail-recursive.  If we erase types and proofs, the function #dyncode("ifact2")
is essentially defined as follows:

#dats2xhtml("\
fun ifact2 (n) = let
  fun loop (n, i, r) =
    if n - i > 0 then let
      val r1 = (i+1) * r in loop (n, i+1, r1)
    end else r // end of [if]
  // end of [loop]
in
  loop (n, 0, 1)
end // end of [ifact2]
")

When the inner function #dyncode("loop") is called on three arguments n, i
and r, the precondition for this call is that i is natural number less than
or equal to n and r equals fact(i), that is, the value of the factorial
function on i. This precondition is captured by the type assigned to
#dyncode("loop") and thus enforced at each call site of #dyncode("loop") in
the implementation of #dyncode("ifact2").

")#comment("para")

#para("\

Please find #mycodelink("CHAP_PRGTHMPRV/ifact23.dats", "on-line")
the entirety of the above presented code plus some testing code.\

")

</sect1><!--"example_another_verified_factorial"-->

<!-- ****** ****** -->

<sect1
id="example_verified_fast_exponentiation">
#title("Example: Verified Fast Exponentiation")

#para("\
Given an integer x, pow(x, n), the nth power of x, can be defined
inductively as follows:

#atscode("\
pow (x, 0) = 1
pow (x, n) = x * pow (x, n-1) (for all n > 0)
")

A direct implementation of this definition is given as follows:

#dats2xhtml("\
fun ipow {n:nat} .<n>.
  (x: int, n: int n): int = if n > 0 then x * ipow (x, n-1) else 1
// end of [ipow]
")

which is of time-complexity O(n) (assuming multiplication is O(1)). A
more efficient implmentation can be given as follows:

#dats2xhtml("\
fun
ifastpow
{n:nat} .<n>.
(
  x: int, n: int n
) : int =
  if n > 0 then let
    val n2 = half(n)
    val i2 = n-(2*n2)
  in
    if i2 > 0 then ifastpow (x*x, n2) else x * ifastpow (x*x, n2)
  end else 1 // end of [if]
// end of [ifastpow]
")

which makes use of the property that pow(x, n) equals pow(x*x, n/2) if n is
even or x * pow(x*x, n/2) if n is odd. This is referred to as fast
exponentiation. Note that #dyncode("ifastpow") is of time-complexity O(log(n)).

")#comment("para")

#para("\

Clearly, what is done above is not restricted to exponentiation on
integers. As long as the underlying multiplication is associative, fast
exponentiation can be employed to compute powers of any given element. In
particular, powers of square matrices can be computed in this way.  I now
present as follows a verified generic implementation of fast exponentiation.

")#comment("para")

#para("\
Handling generic data properly in a verified implementation often requires some
finesse with the type system of ATS. Let us first introduce an abstract type
constructor #stacode("ELT") as follows:

#sats2xhtml("\
sortdef elt = int // [elt] is just an alias for [int]
abst@ype ELT(a:t@ype, x:elt) = a // [x] is an imaginary stamp
")

This is often referred to as #emphasis("stamping"). For each type T and stamp
x, #stacode("ELT(T, x)") is just T as far as data representation is concerned.
The stamps are imaginary and they are solely used for the purpose of
specification. Let us next introduce an abstract prop-type #stacode("MUL") and
a function template #dyncode("mul_elt_elt"):

#sats2xhtml("\
//
absprop MUL(elt, elt, elt) // abstract mul relation
//
fun
{a:t@ype}
mul_elt_elt{x,y:elt}
  (x: ELT(a, x), y: ELT(a, y)): [xy:elt] (MUL(x, y, xy) | ELT(a, xy))
// end of [mul_elt_elt]
//
")

Please do not confuse #stacode("MUL") with the one of the same name that is
declared in #myatscodelink("prelude/SATS/arith_prf.sats", "arith_prf.sats"). To
state that the encoded multiplication is associative, we can introduce the
following proof function:

#sats2xhtml("\
praxi
mul_assoc
{x,y,z:elt}{xy,yz:elt}{xy_z,x_yz:elt}
(
  MUL(x, y, xy), MUL(xy, z, xy_z), MUL(y, z, yz), MUL(x, yz, x_yz)
) : [xy_z==x_yz] void // end of [mul_assoc]
")

The keyword #keycode("praxi") indicates that #dyncode("mul_assoc") is treated as
a form of axiom, which is not expected to be implemented.

")#comment("para")

#para("
The abstract power function can be readily specified in terms of the
abstract prop-type #stacode("MUL"):

#sats2xhtml("\
dataprop
POW (
  elt(*base*), int(*exp*), elt(*res*)
) = // res = base^exp
  | {x:elt}
    POWbas(x, 0, 1(*unit*))
  | {x:elt}{n:nat}{p,p1:elt}
    POWind(x, n+1, p1) of (POW(x, n, p), MUL(x, p, p1))
// end of [POW]
")

As can be expected, generic fast exponentiation is given the following
interface:

#sats2xhtml("\
fun{a:t@ype}
fastpow_elt_int{x:elt}{n:nat}
  (x: ELT(a, x), n: int n): [p:elt] (POW(x, n, p) | ELT(a, p))
// end of [fastpow_elt_int]
")

")#comment("para")

#para("
With the preparation done above, a straightforward implementation of
#dyncode("fastpow_elt_int") can now be presented as follows:

#dats2xhtml("\
implement
{a}(*tmp*)
fastpow_elt_int
  (x, n) = let
//
(*
lemma: (x*x)^n = x^(2n)
*)
extern
prfun
lemma
{x:elt}{xx:elt}{n:nat}{y:elt}
  (pfxx: MUL(x, x, xx), pfpow: POW(xx, n, y)): POW(x, 2*n, y)
//
overload * with mul_elt_elt // [*] loaded with mul_elt_elt
//
in
//
if
n = 0
then let
  val res = mulunit<a> () in (POWbas () | res) // res = 1
end // end of [then]
else let
  val n2 = half n
  val (pfxx | xx) = x * x
  val (pfpow2 | res) = fastpow_elt_int<a> (xx, n2) // xx^n2 = res
  prval pfpow = lemma (pfxx, pfpow2) // pfpow: x^(2*n2) = res
in
  if n=2*n2
    then (pfpow | res)
    else let
      val (pfmul | xres) = x * res in (POWind(pfpow, pfmul) | xres)
    end // end of [else]
end // end of [else]
//
end // end of [fastpow_elt_int]
")

Note that this implementation of #dyncode("fastpow_elt_int") is not
tail-recursive.  The function template #dyncode("mulunit"), which is called to
produce a unit for the underlying multiplication, is assigned the following
interface:

#sats2xhtml("\
fun{a:t@ype} mulunit (): ELT(a, 1(*stamp*))
")

The proof function #dyncode("lemma") simply establishes that pow(x, 2*n)=
pow(x*x, n) for each natural number n.  I have made an implementation of
#dyncode("lemma") available on-line but I suggest that the interested
reader give it a try first to implement #dyncode("lemma") before taking a
look at the given implementation. Note that the following axioms are needed
to implement #dyncode("lemma"):

#sats2xhtml("\
//
praxi
mul_istot // MUL is total
  {x,y:elt} ((*void*)): [xy:elt] MUL(x, y, xy)
//
praxi
mul_isfun // MUL is functional
  {x,y:elt}{z1,z2:elt}(MUL(x, y, z1), MUL(x, y, z2)): [z1==z2] void
//
")

Another interesting (and possibly a bit challenging) exercise is to
implement #dyncode("fastpow_elt_int") in a tail-recursive fashion.

")#comment("para")

#para("\

Please find on-line the two files
#mycodelink("CHAP_PRGTHMPRV/fastexp.sats", "fastexp.sats") and
#mycodelink("CHAP_PRGTHMPRV/fastexp.dats", "fastexp.dats") that contain the
entirety of the above presented code.

")

#para("\

Now we have implemented #dyncode("fastpow_elt_int"). How can it be used?
Please find #mycodelink("CHAP_PRGTHMPRV/test_fastexp.dats", "on-line") an
example in which #dyncode("fastpow_elt_int") is called to implement fast
exponentiation on a 2-by-2 matrix so that the Fibonacci numbers can be computed
in a highly efficient manner.

")

</sect1><!--"example_verified_fast_exponentiation"-->

<!-- ****** ****** -->

</chapter><!--"programming_with_theorem-proving"-->

#comment(" end of [main.atxt] ")

%{
implement main () = fprint_filsub (stdout_ref, "main_atxt.txt")
%}
